{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ine-divider](https://user-images.githubusercontent.com/7065401/92672068-398e8080-f2ee-11ea-82d6-ad53f7feb5c0.png)\n",
    "<hr>\n",
    "\n",
    "### MySQL and MariaDB for Python Developers\n",
    "# Interacting with Python versus MySQL data types\n",
    "\n",
    "In this project, you will explore the interfaces between Python data types and MySQL data types.\n",
    "\n",
    "You will need access to a MySQL installation where you have superuser permissions. If you do not have such access elsewhere, installing to your personal workstation is a good idea.  Alternately, you might wish to use a Docker container for a self-contained installation.  See ` https://hub.docker.com/_/mysql` for details on that option.\n",
    "\n",
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "\n",
    "**Dynamically guessing good schema**\n",
    "\n",
    "Python variables and attributes are dynamically typed, but each individual value has a strict type.  We sometimes are presented with collections of Python collections, and would like to determine good data layout for this data in MySQL automatically.\n",
    "\n",
    "For this task, assume that your Python data is in the form of an iterable of namedtuples.  After analyzing data input, your function should product a MySQL SQL statement to create a proposed table.  Obviously, emphasizing that this inference is a guess is important, since future Pyton data produced in the same application may not be compatible with the schema. In use, the function should operate in a manner similar to the below:\n",
    "\n",
    "```python\n",
    ">>> print(infer_schema(list_of_named_tuples))\n",
    "CREATE TABLE my_records (\n",
    "    a SMALLINT, \n",
    "    b BIGINT DEFAULT NULL,\n",
    "    c DECIMAL(40,25),\n",
    "    d REAL DEFAULT NULL,\n",
    "    e TEXT\n",
    ");\n",
    "```\n",
    "\n",
    "At the least, the proposed schema should be compatible with the data actually encountered.  If you decide it is not possible to unify the data in a single data definition, you should raise an appropriate exception.  Several sample datasets are provided for you to test against.  You should expand these for more robust testing, especially to consider additional edge cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Numbers1(a=1.23, b=Decimal('1.15573'), c=1000000000000, d=2, e=Fraction(22, 7)),\n",
       " Numbers1(a=4.56, b=Decimal('1.155727349790921717935726'), c=-22, d=5, e=Fraction(1, 3)),\n",
       " Numbers1(a=7.89, b=Decimal('1.0'), c=56, d=9, e=Fraction(5, 1))]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from samples import data1, data2, data3, data4, data5, data6\n",
    "# For example...\n",
    "data1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A possible solution**\n",
    "\n",
    "As in other projects, what is suggested is code that might satisfy the task.  A more fleshed out version would certainly address additional special cases and perform more sophisticated inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unify_datatype(vals):\n",
    "    # Floating point?\n",
    "    if all(isinstance(v, float) for v in vals):\n",
    "        # Python is 64-bit, so this is safest assumption\n",
    "        return \"DOUBLE PRECISION\"\n",
    "    \n",
    "    # Integer?\n",
    "    if all(isinstance(v, int) for v in vals):\n",
    "        # Python has unlimited length ints, so have to guess length\n",
    "        size = max(abs(v) for v in vals).bit_length()\n",
    "        if size <= 16:\n",
    "            return \"SMALLINT\"\n",
    "        elif size <= 32:\n",
    "            return \"INTEGER\"\n",
    "        elif size <= 64:\n",
    "            return \"BIGINT\"\n",
    "        else:\n",
    "            from math import log10\n",
    "            return f\"DECIMAL({int(1+log10(2**size))})\"\n",
    "\n",
    "    # Decimal?\n",
    "    from decimal import Decimal\n",
    "    if all(isinstance(v, Decimal) for v in vals):\n",
    "        # The tricky part is how many digits after decimal point\n",
    "        # ... the \"context\" of decimal module is weird!\n",
    "        to_right = 0\n",
    "        to_left = 1\n",
    "        for v in vals:\n",
    "            v_str = str(v)\n",
    "            to_right = max(to_right, v_str[::-1].find('.'))\n",
    "            to_left = max(to_left, v_str.find('.'))\n",
    "        to_right += 1  # Need one more decimal point than calculated\n",
    "        return f\"DECIMAL({to_left+to_right}, {to_right})\"\n",
    "    \n",
    "    # If it is string, is is currency? (for now, only know dollars)\n",
    "    if all(isinstance(v, str) and v[0] == '$' for v in vals):\n",
    "        return \"DECIMAL(10, 2)\"\n",
    "    \n",
    "    # MySQL does not have a Fraction/Rational type. \n",
    "    # What to do with Python Fractions?\n",
    "    # Co-opt the POINT data type with X/Y as numerator/denominator?\n",
    "    # Here we just convert to a float though\n",
    "    from fractions import Fraction\n",
    "    if all(isinstance(v, Fraction) for v in vals):\n",
    "        return \"DOUBLE PRECISION\"  \n",
    "    \n",
    "    # If nothing else can be found, use the raw Python repr for the datatype\n",
    "    return \"TEXT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_schema_nt(dataset):\n",
    "    # May have a lazy iterator, concretize (perhaps optimize this later)\n",
    "    dataset = list(dataset)\n",
    "    # Require at least two rows to infer datatype\n",
    "    if len(dataset) < 2:\n",
    "        raise ValueError(\"At least two rows are required for inference\")\n",
    "    # Check if they are all namedtuples (probably)\n",
    "    if not all(isinstance(t, tuple) and hasattr(t, '_fields') for t in dataset):\n",
    "        raise ValueError(\"The dataset does not appear to be exclusively namedtuples\")\n",
    "    # Check that namedtuples have same type\n",
    "    if not len({o.__class__.__name__ for o in dataset}) == 1:\n",
    "        raise ValueError(\"The dataset has namedtuples of varying types\")\n",
    "\n",
    "    tablename = dataset[0].__class__.__name__\n",
    "    fields = dataset[0]._fields\n",
    "    types = {}\n",
    "\n",
    "    # Can we find a good type for each column?\n",
    "    for n, col in enumerate(fields):\n",
    "        colvals = [row[n] for row in dataset]\n",
    "        if not (coldef := unify_datatype(colvals)):\n",
    "            raise ValueError(f\"Could not find unified datatype for column {col}\")\n",
    "        types[col] = coldef\n",
    "\n",
    "    # Format the DDL SQL command\n",
    "    sql = [f\"CREATE TABLE {tablename} (\"]\n",
    "    for col, typ in types.items():\n",
    "        sql.append(f\"    {col} {typ},\")\n",
    "    sql[-1] = sql[-1].rstrip(',')\n",
    "    sql.append(\");\")\n",
    "    return  \"\\n\".join(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data1\n",
      "CREATE TABLE Numbers1 (\n",
      "    a DOUBLE PRECISION,\n",
      "    b DECIMAL(26, 25),\n",
      "    c BIGINT,\n",
      "    d SMALLINT,\n",
      "    e DOUBLE PRECISION\n",
      ");\n",
      "--------------------------------------------------\n",
      "data2\n",
      "The dataset has namedtuples of varying types\n",
      "--------------------------------------------------\n",
      "data3\n",
      "CREATE TABLE Numbers3 (\n",
      "    a DECIMAL(10, 2),\n",
      "    b DECIMAL(26, 25),\n",
      "    c BIGINT,\n",
      "    d SMALLINT,\n",
      "    e DOUBLE PRECISION\n",
      ");\n",
      "--------------------------------------------------\n",
      "data4\n",
      "The dataset does not appear to be exclusively namedtuples\n",
      "--------------------------------------------------\n",
      "data5\n",
      "At least two rows are required for inference\n",
      "--------------------------------------------------\n",
      "data6\n",
      "CREATE TABLE Numbers1 (\n",
      "    a DOUBLE PRECISION,\n",
      "    b DECIMAL(26, 25),\n",
      "    c DECIMAL(21),\n",
      "    d SMALLINT,\n",
      "    e DOUBLE PRECISION\n",
      ");\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for data in \"data1 data2 data3 data4 data5 data6\".split():\n",
    "    print(data)\n",
    "    data = eval(data)\n",
    "    try:\n",
    "        print(infer_schema_nt(data))\n",
    "    except ValueError as err:\n",
    "        print(err)\n",
    "    print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)\n",
    "\n",
    "## Part 2\n",
    "\n",
    "**Working with other Python collections**\n",
    "\n",
    "No solution is provided here, but you should be able to re-use most of the work with namedtuples.  Consider how you would need to change the table inference if you are given an iterable of dictionaries? What about an iterable of data classes? What about an iterable of plain lists or tuples. What about and iterable of custom Python objects with various attributes.\n",
    "\n",
    "What would be reasonable exception checking if you wished to use a iterable of heterogeneous Python \"data objects\"? In some ways, it might be reasonable to consider a namedtuple data class, or dictionary \"morally equivalent\" from the point-of-view of a PostgreSQL table.  What limits are likely to apply.\n",
    "\n",
    "The solution provided to part 1 did not consider NULLable columns. It might be reasonable to look for Python `None` values in the data set and use that as guidance for being NULLable.  Moreover, if dictionaries or other mappings are the source data, it *might* (or might not) be appropriate to treat a missing key as a NULL value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notes on a solution**\n",
    "\n",
    "All the logic in the function `unify_datatype()` should remain identical when working with other \"data objects\" in Python.  The aspects that will differ when working with objects other than namedtuples are how to identify the column names, how to validate compatible structures, how to extract the data corresponding to one column, and so on.  \n",
    "\n",
    "Once you find \"all the objects that might go in this column\", you can use the identical `unify_datatype()` function.  It is worthwhile to think about how to make that function more robust or general, of course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![orange-divider](https://user-images.githubusercontent.com/7065401/92672455-187a5f80-f2ef-11ea-890c-40be9474f7b7.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
